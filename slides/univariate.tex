\begin{frame}\frametitle{Univariate data analysis: What we will cover}
	\includegraphics[width=\textwidth]{\imagedir/mindmaps/univariate-section-mapping.png}
\end{frame}

\begin{frame}\frametitle{Usage examples for this section}
	\begin{itemize}
		\item	\emph{Co-worker}: Here are the yields from a batch system for the last 3 years (1256 data points)
		\begin{itemize}
			\item	what sort of distribution do the data have?
			\item	yesterday's yield was less than 160 g/L, what are the chances of that?
		\end{itemize}
		\item	\emph{Yourself}: Using historical failure rate data of the pumps, what is the probability that 3 pumps will fail this month?
		\item	\emph{Manager}: does reactor 1 have better final product purity, on average, than reactor 2?
		\item	\emph{Colleague}: what is the 95\% confidence interval for the density of your powder ingredient?
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{References and readings}
	\begin{itemize}
		\item	Any standard statistics text book
		\item	\textbf{Recommended}: Box, Hunter and Hunter, \emph{Statistics for Experimenters}, Chapter 2 (both 1st and 2nd edition)
		\item	\textbf{Wikipedia articles} on any term that seems unfamiliar or that you've forgotten

		\vspace{2cm}
		\item	Hodges and Lehmann, \emph{Basic Concepts of Probability and Statistics}
		\item	Hogg and Ledolter, \emph{Engineering Statistics}
		\item	Montgomery and Runger, \emph{Applied Statistics and Probability for Engineers}
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Concepts: use this as a checklist at the end of this section}
	\begin{center}
		\includegraphics[height=0.9\textheight]{\imagedir/mindmaps/univariate-section-concepts.png}
	\end{center}
\end{frame}

\begin{frame}\frametitle{Our goal with Engineering Statistics}
	\begin{exampleblock}{We try to answer the question:}
		``What happened?''
	\end{exampleblock}
\end{frame}

\begin{frame}\frametitle{Variability: Life is pretty boring without variability}

	\begin{center}
		\includegraphics[width=\textwidth]{\imagedir/concepts/variation/variation-none.png}
	\end{center}
	(and this course would be unnecessary)
\end{frame}

\begin{frame}\frametitle{We have plenty of variability in our recorded data}

	\begin{center}
		\includegraphics[width=\textwidth]{\imagedir/concepts/variation/variation-some.png}
	\end{center}
	\begin{itemize}
		\item	Other unaccounted for sources, often called \textbf{\emph{error}}
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Variability is created from many sources, sometimes intentionally!}
	\begin{itemize}
		\item	Feedback control: introduces variability
		\item	Operating staff: introduce variability into a process
		\item	Sensor drift, spikes, noise, recalibration shifts, errors in our sample analysis
	\end{itemize}

	\includegraphics[width=\textwidth]{\imagedir/concepts/variation/variation-more.png}
\end{frame}

\begin{frame}\frametitle{Variability due to unintentional circumstances}
	\begin{itemize}
		\item	Raw material properties are not constant
		\item	Production disturbances:
		\begin{itemize}
			\item	external conditions change (ambient temperature, humidity)
			\item	equipment breaks down, wears out, maintenance shut downs
		\end{itemize}
	\end{itemize}

	\includegraphics[width=\textwidth]{\imagedir/concepts/variation/variation-spikes.png}

	All this variability keep us process engineers employed, but it comes at a price.
\end{frame}

\begin{frame}\frametitle{The high cost of variability in your final product}
	\begin{block}{\color{red} Assertion}
		\begin{center}
			Customers expect both \textbf{\color{myGreen}uniformity} and \textbf{\color{myGreen}low cost} when they buy your product. Variability defeats both objectives.
		\end{center}
	\end{block}
	
	\pause
	\begin{enumerate}
		\item	Customer totally unable to use your product. Examples:
		\begin{itemize}
			\item	fresh milk,
			\item	viscosity too high,
			\item	oil that causes pump failure
		\end{itemize}
		\pause
		\item	Your product leads to poor performance. Example:
		\begin{itemize}
			\item	customer must put in more energy than usual (melting point too high)
			\item	longer reaction times for off-spec catalyst
		\end{itemize}
		\pause
		\item	Your brand is diminished
		\begin{itemize}
			\item	Extreme example: Maple Leaf Foods (listeriosis outbreak in 2008)
			\item	Toyota quality issues with accelerators, early 2010
		\end{itemize}
	\end{enumerate}
\end{frame}

\begin{frame}\frametitle{The high cost of variability in your final product}
	Variability also has these costs:
	
	\vspace{12pt}
	\begin{enumerate}
		\item	Inspection costs:
		\begin{itemize}
			\item	too expensive and inefficient to test every product
			\item	low variability means you don't need to inspect every product
		\end{itemize}
		\item	Off-specification products cost you and customer money:
		\begin{itemize}
			\item	reworked
			\item	disposed
			\item	sold at a loss
		\end{itemize}
	\end{enumerate}
\end{frame}

\begin{frame}\frametitle{The cost of variability in your raw materials: it reaches your final product}
	
	\centerline{\includegraphics[height=0.9\textheight]{\imagedir/concepts/variation/feedback-control-variance-reduction.png}}
	\begin{columns}[T]
		\column{0.30\textwidth}
		\vspace{-2.5cm}
		\begin{exampleblock}{}
			{\color{myGreen} Variability affects you: whether you are on the receiving end or supplying end.}
		\end{exampleblock}	
		\column{0.70\textwidth}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{This entire course is about variability}
	\begin{enumerate}
		\item	Visualization: \emph{\color{blue}show} the variability
		\item	This section: \emph{\color{blue}quantify} variability, then \emph{\color{blue}compare} variability
		\item	Section 3: how variation in one variable \emph{\color{blue}affects} another (least squares)
		\item	Section 4: we intentionally \emph{\color{blue}introduce} variation to learn more about our process through designed experiments (DOE)
		\item	Section 5: construct monitoring charts to \emph{\color{blue}track} variability
		\item	Section 6: dealing with multiple variables, simultaneously extracting information (latent variables)
	\end{enumerate}
\end{frame}

\begin{frame}\frametitle{Histograms ask the reader to infer the long term expectation of a variable}
	\begin{columns}
		\column{0.8\textwidth}
			\includegraphics[height=0.9\textheight]{\imagedir/univariate/simulate-CLT-dice-slides.png}
		\column{0.3\textwidth}
			\includegraphics[width=0.9\textwidth]{\imagedir/univariate/6sided_dice-Wikipedia.png}
			
			\see{Wikipedia}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Histograms summarize variation in a measured variable}

	Shows \emph{number} of samples that occur in a \emph{\textbf{category}}: called a {\color{purple}{\textbf{frequency distribution}}}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{\imagedir/univariate/histogram-children-by-gender.png}
	\end{center}
\end{frame}

\begin{frame}\frametitle{Histograms for a \textbf{continuous} variable use category bins (usually of equal size)}
	\begin{center}
		\includegraphics[width=0.6\textwidth]{\imagedir/univariate/histogram-package-mass.png}
	\end{center}
	We are overfilling to achieve the 1 kg spec. If we can reduce variation: \emph{boss gives us a raise!}
	
\end{frame}

\begin{frame}\frametitle{Plot your expectation of these histograms:}
	 \vspace{2cm}
	
	\begin{columns}
		\column{0.8\textwidth}
			\begin{itemize}
				\item	repeated lab measurement to measure \\$g$ (9.81 $m/s^2$)
				\item	this class's grades for a really easy test
				\item	annual income for people in Canada
				\item	bacterial count [number per cubic inch] from deli meat samples
			\end{itemize}
		\column{0.3\textwidth}
			\includegraphics[width=0.9\textwidth]{\imagedir/univariate/6sided_dice-Wikipedia.png}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Histograms are all about inferring long-term probabilities}
	You implicitly used a ``long'' time scale for your prior histograms

	\vspace{12pt}
	\textbf{Other examples:}
	\begin{itemize}
		\item	Next throw on a die? A fair die has a 16.67\% chance of showing each number.
		\item	What is the batch yield tomorrow? Long term average $160~\text{g.L}^{-1} \pm 20~\text{g.L}^{-1}$
		\item	Boy or a girl? Canada's sex ratio at birth 1.06:1 (boy:girl)
		\item	Age at death? Canadian life tables: e.g. in 2002, for females:
		\begin{itemize}
			\item	have a 98.86\% chance of reaching age 30
			\item	have a 77.5\% chance of reaching age 75
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Frequency distribution}

	See the printed notes for detailed steps to construct a frequency distribution.
	\begin{itemize}
		\item	Categorical variables
		\item	Continuous variable: with and without quantization
	\end{itemize}

	Main steps are:
	\begin{enumerate}
		\item	Decide what you are measuring
		\item	Resolution for the measurement ($x$) axis
		\item	Find number of observations in each bin
		\item	Plot it as a bar plot.
	\end{enumerate}

	If you divide bin count by $N$, then you are plotting the \emph{relative frequency}.
\end{frame}

\begin{frame}\frametitle{Data from a reactor: production yield, measured in g/L}
	\begin{columns}
		\column{0.6\textwidth}
			\includegraphics[width=\textwidth]{\imagedir/univariate/batch-yields-time-series.png}
		\column{0.5\textwidth}
			\includegraphics[width=\textwidth]{\imagedir/univariate/batch-yields-slides.png}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Data from a reactor: production yield, measured in g/L}
	\begin{columns}
		\column{0.5\textwidth}
			\includegraphics[width=\textwidth]{\imagedir/univariate/batch-yields-good-slides.png}
		\column{0.5\textwidth}
			\onslide+<2->{
				\includegraphics[width=0.9\textwidth]{\imagedir/univariate/batch-yields-poor-slides.png}
			}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Frequency distribution: you can also use a relative frequency}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{\imagedir/univariate/frequency-histogram.png}
	\end{center}
	{\color{purple}{\textbf{Relative frequency}}}
	\begin{itemize}
		\item	does not require knowing $N$
		\item	can be compared to other distributions (e.g. \emph{side-by-side})
		\item	for large $N$ it quickly resembles the population's distribution
		\item	area under histogram is equal to 1 (related to probability)
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Unpaid housework in two cities: fair comparison are harder with frequency data alone}
	\begin{columns}
		\column{0.5\textwidth}
			\includegraphics[width=0.9\textwidth]{\imagedir/univariate/histogram-housework-hamilton.png}
		\column{0.5\textwidth}
			\onslide+<2->{
				\includegraphics[width=0.9\textwidth]{\imagedir/univariate/histogram-housework-hamilton-relative.png}
			}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Unpaid housework in two cities: fair comparison are harder with frequency data alone}
	\begin{columns}
		\column{0.5\textwidth}
			\includegraphics[width=0.9\textwidth]{\imagedir/univariate/histogram-housework-hamilton.png}
		\column{0.5\textwidth}
			\onslide+<2->{
				\includegraphics[width=0.9\textwidth]{\imagedir/univariate/histogram-housework-toronto.png}
			}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Unpaid housework in two cities: easier comparison with {\color{myOrange} \emph{relative}} frequency data}
	\begin{columns}
		\column{0.5\textwidth}
			\includegraphics[width=0.9\textwidth]{\imagedir/univariate/histogram-housework-hamilton-relative.png}
		\column{0.5\textwidth}
			\onslide+<2->{
				\includegraphics[width=0.9\textwidth]{\imagedir/univariate/histogram-housework-toronto-relative.png}
			}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Income of people in Ontario, 2008}
	\begin{columns}
		\column{0.5\textwidth}
			\includegraphics[width=\textwidth]{\imagedir/univariate/Income-Ontario-2008.png}\
			
			\onslide+<2->{
			
			\hfill Rotated here for better reading $\longrightarrow$
			}
		\column{0.5\textwidth}
			\onslide+<2->{
				\includegraphics[height=0.9\textheight]{\imagedir/univariate/Income-Ontario-2008-rotated.png}
			}
		
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Histogram creation in R is very straightforward}
	\includegraphics[height=0.7\textheight]{\imagedir/univariate/furnace-data-analysis.png}
\end{frame}

\begin{frame}\frametitle{We introduce many concepts in this module; many of which you know already}
	\vspace{-16pt}
	\begin{center}
		\includegraphics[height=0.9\textheight]{\imagedir/mindmaps/univariate-section-concepts.png}
	\end{center}
\end{frame}

\begin{frame}\frametitle{Review of terminology you are already comfortable with}
	{\color{purple}{\textbf{Population}}}

	Large collection of \emph{potential} measurements (not necessary to be infinite, a large $N$ works well)

	\onslide+<2->{
	{\color{purple}{\textbf{Sample}}}

	Collection of observations that have actually occurred ($n$ samples)

	In engineering: usually have plenty of data; that often is our population.
	}

\onslide+<3->{
	\begin{center}
		\includegraphics[width=.8\textwidth]{\imagedir/univariate/batch-yields.png}
	\end{center}
}
\end{frame}

\begin{frame}\frametitle{Quick review of basic terminology}
	{\color{purple}{\textbf{Probability}}}

	Area under relative frequency distribution curve equals 1.

	\vspace{24pt}
	Probability is the fraction of the area under the curve.

	\vspace{24pt}
	{\color{myGreen}Construct histograms for yourself}, then mark the area:
	\begin{itemize}
		\item	Probability of grades less than 80\%
		\item	Probability that number on dice is 1 or 2
		\item	Bacterial count per cubic inch is above 10,000
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Use of the area under a relative frequency histogram}
	{\color{purple}{\textbf{Probability}}}

	\begin{columns}
		\column{0.5\textwidth}
			\includegraphics[width=\textwidth]{\imagedir/univariate/simulate-CLT-dice-slides.png}
		\column{0.5\textwidth}
			\includegraphics[width=\textwidth]{\imagedir/univariate/batch-yields-slides.png}
	\end{columns}	
\end{frame}

\begin{frame}\frametitle{Use of the area under a relative frequency histogram}
	{\color{purple}{\textbf{Probability}}}

	\begin{columns}
		\column{0.5\textwidth}
			\includegraphics[width=\textwidth]{\imagedir/univariate/simulate-CLT-dice-slides-coloured.png}
		\column{0.5\textwidth}
			\includegraphics[width=\textwidth]{\imagedir/univariate/batch-yields-slides-coloured.png}
	\end{columns}	
\end{frame}

\begin{frame}\frametitle{Quick review of basic terminology}
	{\color{purple}{\textbf{Parameter}}}
	\begin{itemize}
		\item	Value that describes the population's \emph{distribution} (fixed)
	\end{itemize}

	{\color{purple}{\textbf{Statistic}}}
	\begin{itemize}
		\item	An estimate of one of the population's parameters (estimate implies that it varies)
	\end{itemize}
	
	\onslide+<2->{

	{\color{purple}{\textbf{Mean}}}
	\begin{itemize}
		\item	Measure of location (position)
	\end{itemize}

	$
	\begin{array}{rcl}
		\text{Population mean:} \qquad \mathcal{E}\left\{x \right\} = &\mu &= \displaystyle \frac{1}{N}\sum{x} \\
		\\
		\text{Sample mean:} \qquad &\bar{x} &= \displaystyle \frac{1}{n}\sum_{i=1}^{n}{x_i}
	\end{array}
	$
	\begin{itemize}
		\item	Mean by itself is not sufficient to describe a (random) variable
	\end{itemize}
	}
\end{frame}

\begin{frame}\frametitle{Quick review of basic terminology}
	{\color{purple}{\textbf{Variance (spread)}}}
	\begin{itemize}
		\item	Measure of spread, or variability
		\item	Population variance:
	$
	\begin{array}{rcl}
		\mathcal{V}\left\{x\right\} = \mathcal{E}\left\{ (x - \mu )^2\right\} &=& \sigma^2 \\
		&=& \displaystyle \frac{1}{N}\sum{(x-\mu)^2} \\
	\end{array}
	$
	\onslide+<2->{
		\item	The population {\color{purple}standard deviation} $= \sigma$
	}
	\end{itemize}
	\begin{itemize}
		\onslide+<3->{
		\item	Sample variance:
		$
		\begin{array}{rcl}
			s^2 &=& \displaystyle \frac{1}{n-1}\sum_{i=1}^{n}{(x_i - \bar{x})^2}
		\end{array}
		$
		}
		\onslide+<4->{
		
		\item	The sample {\color{purple}standard deviation} $= s$
		}
	\end{itemize}
	\onslide+<5->{
		{\color{purple}{\textbf{Degrees of freedom}}}: the denominator term, $n-1$
	}
\end{frame}

\begin{frame}\frametitle{Quick review of basic terminology}
	{\color{purple} \textbf{Outlier}}
	\vspace{1cm}
	
	\onslide+<2->{
		\begin{exampleblock}{}
			\begin{center}
				\emph{A point that is unusual, given the context of the surrounding data}
			\end{center}
		\end{exampleblock}
	}
	\onslide+<3->{
		\begin{itemize}
			\item	4024, 5152, 2314, 6360, 4915, 9552, 2415, 6402, 6261
			\item	4, 61, 12, 64, 4024, 52, -8, 67, 104, 24
		\end{itemize}
	}
	\onslide+<4->{
		\vspace{12pt}
		Let's look at the sample statistics:
		
		\begin{itemize}
			\item	{\color{myOrange}Regular:} \qquad$~\bar{x} = 440.4$ ~~~~\qquad $s = 1260$
			\item	{\color{myOrange}Robust:}  Median = 56.5 \qquad MAD = 57
		\end{itemize}
	}
\end{frame}

\begin{frame}\frametitle{Outliers are not only univariate; we also have multivariate outliers}
	\centerline{\includegraphics[height=0.9\textheight]{\imagedir/univariate/multivariate-outliers.png}}
\end{frame}

\begin{frame}\frametitle{Quick review of basic terminology}
	{\color{purple}{\textbf{Median (location)}}}
		
	\begin{itemize}
		\onslide+<2->{
			\item	One alternative measure of central tendency.
			\item	Robust statistic: insensitive (robust) to outliers in the data
			\item	The mean is extremely sensitive to outliers
		}
		\onslide+<3->{
			\item	The most robust estimator of sample location:
			\begin{itemize}
				\item	breakdown of 50\%: implies 50\% of the data contaminated before median breaks down
				\item	breakdown of the mean is $1/n$\%:  only 1 outlier required
			\end{itemize}
		}
	\end{itemize}
	\onslide+<4->{
		\begin{exampleblock}{Why should you care?}
			Always use the median and MAD (described next) where possible.
		\end{exampleblock}
	}
\end{frame}

\begin{frame}\frametitle{{\color{purple}{\textbf{Median absolute deviation, MAD (spread)}}}}
	
	\begin{itemize}
		\item	A robust measure of spread
	\end{itemize}
	$ \qquad\qquad \text{mad}\left\{ x_i \right\} = c \cdot \text{median}\left\{ \| x_i - \text{median}\left\{ x_i \right\} \| \right\} $
	\onslide+<2->{
		\begin{itemize}
			\item	The constant $c = 1.4826$ makes MAD consistent with the standard deviation if $x_i$ are normally distributed.
			\item	Breakdown of MAD: 50\%
			\item	Breakdown of standard deviation: $1/n$\%
		\end{itemize}
	}
	\onslide+<3->{
	\begin{itemize}
		\item	Read the paper by Rousseeuw {\scriptsize (especially 600-level and graduate students)}:
		
			\begin{itemize}
				\item	\href{http://yint.org/robust-tutorial}{http://yint.org/robust-tutorial}
			\end{itemize}

		\end{itemize}
	
		\begin{itemize}
			\item	Another robust estimate of spread is the IQR (inter-quartile range), but not as robust as the MAD
		\end{itemize}
	}
\end{frame}

\begin{frame}\frametitle{We will take a look at some basic distributions next}
	\begin{itemize}
		\item	Just a review; please read any of the books listed on the website for more details
		\item	Focus on when to use the distribution
		\item	And how the distribution looks
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{The \textbf{binary} (Bernoulli distribution)}
	For binary events: \textbf{event A} and \textbf{event B}
	\begin{columns}
		\column{6cm}
		\begin{itemize}
			\item	Pass/fail, or yes/no system
			\item	$p(\text{pass}) + p(\text{fail}) = 1$
			\item	Example: what is probability of seeing this sequence if $p(\text{pass}) = 0.7$
			\item	pass, pass, pass
			\begin{itemize}
				\item	$(0.7)(0.7)(0.7) = 0.343$
				\item	\emph{only if each ``pass'' or ``fail''} is independent of the other samples
			\end{itemize}
			\item	pass, fail, pass, fail, pass, fail
			\begin{itemize}
				\item	$(0.7)(0.3)(0.7)(0.3)(0.7)(0.3)$
			\end{itemize}
		\end{itemize}
		\column{5cm}
		\includegraphics[width=\textwidth]{\imagedir/univariate/histogram-70-30.png}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{The \textbf{uniform} distribution}
	Each outcome is equally as likely to occur as all the others. The classic example is dice: each face is equally as likely.

	Probability distribution for an event with 4 possible outcomes:
	\begin{center}
		\includegraphics[width=0.4\textwidth]{\imagedir/univariate/histogram-4-cuts.png}
	\end{center}
\end{frame}

\begin{frame}\frametitle{Introduction to the \textbf{normal} distribution}

	We need to look at two topics first:
	\begin{enumerate}
		\item	The Central Limit Theorem
		\item	The concept of \emph{Independence}
	\end{enumerate}
\end{frame}

\begin{frame}\frametitle{Central limit theorem, \textbf{and} the important assumptions that go with it}
	\begin{exampleblock}{\color{purple}{Central limit theorem}}
		
		
			The average of a sequence of independent values from \emph{any distribution} will approximate the normal distribution, provided the original distribution has finite variance.
		
	\end{exampleblock}
	\onslide+<2->{
		\includegraphics[width=0.9\textwidth]{\imagedir/univariate/CLT-derivation.png}
		
		\textbf{Assumption}: the samples used to compute the average are independent.
	}
	
	\vspace{4pt}
	\onslide+<3->{
		\hrule
		\vspace{4pt}
		How is this theorem relevant with engineering data?
		\begin{itemize}
			\item	In many cases we are using averages from our raw data
			\item	These raw data are almost never normally distributed
			\item	But using the assumption that these averages are normally distributed is powerful
		\end{itemize}
	}
\end{frame}

\begin{frame}\frametitle{}
	\vspace{-5.8cm}
	\includegraphics[width=\textwidth]{\imagedir/univariate/simulate-CLT.png}
	
	\vspace{-9cm}
	\hfill \includegraphics[width=0.1\textwidth]{\imagedir/univariate/6sided_dice-Wikipedia.png}
\end{frame}

\begin{frame}\frametitle{The assumption of \textbf{independence} is widely used}

	It is a condition for the Central Limit Theorem.
	\begin{block}
		{\color{purple}{Independence}}
		\begin{center}
			The samples are \emph{randomly} taken from a population. If two samples are independent, there is no possible relationship between any two samples.
		\end{center}
	\end{block}
	\onslide+<2->{

		\vspace{12pt}
		{\color{myOrange}We frequently violate this assumption in engineering work. }
	}
	
	\onslide+<3->{
		\vspace{6pt}
		Discuss these examples:
	}
	\begin{itemize}
		\onslide+<3->{
			\item	The snowfall, recorded in millimetres, every 6 hours, for 1 week.
			\item	Snowfall, recorded on 15 January for every year since 1976: independent or not?
		}
		\onslide+<4->{
			\item	A questionnaire is given to students. Are the answers independent if students discuss the questionnaire prior to handing it in?
			\item	The impurity values in the last 10 batches of product produced.
			\item	See the exercise in the notes about independence (pump failures).
		}
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Which one of these series have independent values in time?}
	\begin{columns}[t]
		\column{1.2\textwidth}
			\includegraphics[width=1.\textwidth]{\imagedir/univariate/simulate-independence.png}
		\column{0.20\textwidth}
	\end{columns}
	
	% The middle one
	% The lower one is negatively autocorrelated (and the strongest negative autocorr)
	% The first ine is weakly positively autocorrelated
\end{frame}


\begin{frame}\frametitle{Recap of the central limit theorem from the prior video}
	%\includegraphics[width=0.9\textwidth]{\imagedir/univariate/CLT-derivation-part1.png}
	\includegraphics[width=0.9\textwidth]{\imagedir/univariate/CLT-derivation-part2.png}
\end{frame}

\begin{frame}\frametitle{The basic normal distribution for a standardized variable}
	\begin{center}
		\includegraphics[width=0.85\textwidth]{\imagedir/univariate/normal-distribution-standardized-z.png}
	\end{center}
\end{frame}

\begin{frame}\frametitle{The basic normal distribution for a standardized variable}
	\begin{center}
		\includegraphics[width=0.85\textwidth]{\imagedir/univariate/normal-distribution-standardized-z-coloured.png}
		%\includegraphics[width=0.85\textwidth]{\imagedir/univariate/normal-distribution-standardized-z-coloured-right.png}
	\end{center}
\end{frame}

\begin{frame}\frametitle{The ammonia dataset: the raw data and the effect of centering the data}
	\begin{columns}[T]
		\column{0.5\textwidth}
			\includegraphics[width=\textwidth]{\imagedir/univariate/Ammonia-histograms-1.png}
			
			The raw data: $x$
			
			The mean ammonia concentration is $ \overline{x}=36$.
		\column{0.5\textwidth}
			\onslide+<2->{
				\includegraphics[width=\textwidth]{\imagedir/univariate/Ammonia-histograms-2.png}
			
				The raw data after centering: $x - \overline{x}$
			}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{The ammonia dataset: the centered data and the effect of scaling}
	\begin{columns}[T]
		\column{0.5\textwidth}
			\includegraphics[width=\textwidth]{\imagedir/univariate/Ammonia-histograms-2.png}
			
			These are the centered  data: $x - \overline{x}$
			
			The standard deviation $ s=8.5$.
			
			\vspace{12pt}
			Note: the parameter, $\sigma$, is unknown.
			
			The statistic, $s=8.5$ is an estimate of $\sigma$.
		\column{0.5\textwidth}
			\onslide+<2->{
				\includegraphics[width=\textwidth]{\imagedir/univariate/Ammonia-histograms-3.png}
			
				These are the data after scaling: $\dfrac{x - \overline{x}}{s}$
				
				\vspace{12pt}
				$\dfrac{x - \overline{x}}{s} = \dfrac{1}{s}\cdot\left(x - \overline{x}\right)$
			}
	\end{columns}
\end{frame}

% \begin{frame}\frametitle{The ammonia dataset: the centered data and the effect of scaling}
% 	\begin{columns}[T]
% 		\column{0.5\textwidth}
% 			\includegraphics[width=\textwidth]{\imagedir/univariate/Ammonia-histograms-2.png}
%
% 			These are the centered  data: $x - \overline{x}$
%
% 			The standard deviation $ s=8.5$.
%
% 			\vspace{12pt}
% 			Note: the parameter, $\sigma$, is unknown.
%
% 			The statistic, $s=8.5$ is an estimate of $\sigma$.
% 		\column{0.5\textwidth}
%
% 				\includegraphics[width=\textwidth]{\imagedir/univariate/Ammonia-histograms-3.png}
%
% 				If we had population variables, we write: \[z = \dfrac{x - \mu}{\sigma}\]
%
% 	\end{columns}
% \end{frame}

\begin{frame}\frametitle{The {\color{purple}{standard form}} definition}
	\begin{columns}[T]
		\column{0.5\textwidth}
		For \emph{\color{myGreen}parameters}: 
			
			\[z = \dfrac{x - \mu}{\sigma}\]
		\column{0.5\textwidth}
		For \emph{\color{myGreen}statistics}: 
			
			\[z = \displaystyle \frac{x - \overline{x}}{s}\]
	\end{columns}
	
	\vspace{36pt}
	\begin{itemize}
		\item	They are both the same structure.
		%\item	${\color{red} \mathbf{x}} \sim \mathcal{N}(\mu, \sigma^2)$ \hfill {\color{myOrange}{$\longleftarrow$ do not use the standard deviation, use $\sigma^2$}}
		%\item	$z \sim \mathcal{N}(0.0, 1.0)$ after standardization
		\item	The units are removed by cancellation.
		\item	Standardization allows us to straightforwardly compare 2 variables that have different means and spreads
		%\item	{\color{red}{Recommendation}}: make sure you can read a statistical table (see end of Chapter 2)
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Two approximate rules you must remember: 70\% rule}
	\begin{center}
		\includegraphics[width=0.85\textwidth]{\imagedir/univariate/normal-distribution-standardized-z-no-labels-70-percent-area.png}
	\end{center}
\end{frame}

\begin{frame}\frametitle{Two approximate rules you must remember: 95\% rule}
	\begin{center}
		\includegraphics[width=0.85\textwidth]{\imagedir/univariate/normal-distribution-standardized-z-no-labels-95-percent-area.png}
	\end{center}
\end{frame}

\begin{frame}\frametitle{The {\color{purple}{standard form}} definition}
	\begin{columns}[T]
		\column{\textwidth}
		
			
			\[z = \dfrac{x - \mu}{\sigma}\]
		
	\end{columns}
	
	\vspace{36pt}
	\begin{itemize}
		\item	${\color{red} \mathbf{x}} \sim \mathcal{N}(\mu, \sigma^2)$ \hfill {\color{myOrange}{$\longleftarrow$ do not use the standard deviation, use $\sigma^2$}}
		\item	$z \sim \mathcal{N}(0.0, 1.0)$ after standardization

		
		\vspace{36pt}
		\onslide+<2->{
			\item	Height of Australians = $h \sim \mathcal{N}(180\,\text{cm}, 36\,\text{cm}^2)$
		}

	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Integrating the areas under the curve}
	%\centerline{	\includegraphics[width=\textwidth]{\imagedir/univariate/normal-distribution-standardized-z-no-labels.png}	}
	%\centerline{	\includegraphics[width=\textwidth]{\imagedir/univariate/normal-distribution-standardized-z-no-labels-total-area.png}	}
	\centerline{	\includegraphics[width=\textwidth]{\imagedir/univariate/normal-distribution-standardized-z-no-labels-area-50-percent.png}	}
\end{frame}

\begin{frame}\frametitle{Statistical tables (graphical)}
	\begin{columns}[T]
		\column{0.5\textwidth}
			\includegraphics[width=\textwidth]{\imagedir/univariate/Statistical-tables/show-pnorm-and-qnorm-left.png}
		\column{0.5\textwidth}
			\onslide+<2->{
				\includegraphics[width=\textwidth]{\imagedir/univariate/Statistical-tables/show-pnorm-and-qnorm-right.png}
			}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Statistical tables (tabular form)}
	\begin{columns}[T]
		\column{0.5\textwidth}
			\includegraphics[width=0.85\textwidth]{\imagedir/univariate/Statistical-tables/cumulative-normal-table-left.png}
		\column{0.5\textwidth}
			\onslide+<2->{
				\includegraphics[width=0.85\textwidth]{\imagedir/univariate/Statistical-tables/cumulative-normal-table-right.png}
			}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Inverse cumulative distribution (graphical and tabular form)}
	\begin{columns}[T]
		\column{0.5\textwidth}
			\includegraphics[width=0.85\textwidth]{\imagedir/univariate/Statistical-tables/show-pnorm-and-qnorm-right.png}
		\column{0.5\textwidth}
			\includegraphics[width=0.85\textwidth]{\imagedir/univariate/Statistical-tables/cumulative-normal-table-right.png}
	\end{columns}
	Make sure you can use both R (left) or tables (right).
\end{frame}

\begin{frame}\frametitle{Integrating the areas under the curve between two points}
	\centerline{	\includegraphics[width=\textwidth]{\imagedir/univariate/normal-distribution-standardized-z-no-labels-area-up-to-minus-1.png}	}
	
	\emph{Left point}
\end{frame}

\begin{frame}\frametitle{Integrating the areas under the curve between two points}
	\centerline{	\includegraphics[width=\textwidth]{\imagedir/univariate/normal-distribution-standardized-z-no-labels-area-up-to-plus-1.png}	}
	
	\emph{Right point}
\end{frame}

\begin{frame}\frametitle{Integrating the areas under the curve between two points}
	\centerline{	\includegraphics[width=\textwidth]{\imagedir/univariate/normal-distribution-standardized-z-no-labels-area-up-to-plus-1-overlap.png}	}
	
	\emph{Subtract out the darker overlap region}
\end{frame}

\begin{frame}\frametitle{Integrating the areas under the curve between two points}
	\centerline{	\includegraphics[width=\textwidth]{\imagedir/univariate/normal-distribution-standardized-z-no-label-plus-minus-one.png}	}
	
	\emph{Leaving the net result}
\end{frame}

\begin{frame}\frametitle{Two more worked examples to check your knowledge}
	\begin{enumerate}
		\item	Assume $x$ = biological activity of a drug, $x \sim \mathcal{N}(26.9, 9.3)$ 
				
				\vspace{12pt}
				What is the probability of $x \leq 30.0$?
	\end{enumerate}
	\vspace{24pt}
\onslide+<2->{	
	\begin{itemize}
		\item	First create $z$ variable: $z \approx \dfrac{30 - 27}{\sqrt{9}} = \dfrac{3}{3} = 1$
		\item	Find the area under the standard normal distribution: $z \leq 1$
		\onslide+<3->{
			\item	So the area $\approx 84\%$
		}
	\end{itemize}
}
	% Q1: sigma = 3 units; mean = 27; so at 30, we cover all the area between +/- 1sigma, which is 70%. Then add the area from -Inf to -1Sigma = 15%, to 70% + 15% = 85% (or read it from the tables where z=+1)
\end{frame}

\begin{frame}\frametitle{Two more worked examples to check your knowledge}
	\begin{columns}[T]
		
		\column{0.7\textwidth}
			\begin{enumerate}
					\setcounter{enumi}{1}
		
					\item	Assume $x$ = yield from batch process:
						$$x \sim \mathcal{N}(85 \text{~g/L}, 16\, \text{g}^2\text{.L}^{-2})$$
						What is the proportion of yield values between 77 and 93 g/L?
			\end{enumerate}
			% Q2: sigma = 4 units; mean = 85; z = (77-85)/4 = -2;   z=(93 - 85)/4 = 8/4 =2; so area between -2 and +2 = 95%
			\vspace{24pt}
			\begin{itemize}
				\onslide+<2->{
					\item	Create both $z$ variables: $z \approx \dfrac{x - 85}{\sqrt{16}}$
					\item	We get $z$ values of $-2$ and $+2$			
					\item	Area from $-\infty$ to $+2$ is 97.73\%
					\item	Area from $-\infty$ to $-2$ is 2.27\%
				}
				\onslide+<3->{
					\item	Find the area between $-2$ and $+2$ as $\approx 95\%$
				}
			\end{itemize}
		\column{0.4\textwidth}

			\includegraphics[width=\textwidth]{\imagedir/univariate/Statistical-tables/cumulative-normal-table-left.png}
	\end{columns}
	
	
\end{frame}

\begin{frame}\frametitle{Checking for normality. Are these samples normally distributed?}
	\begin{columns}[t]
		\column{0.35\textwidth}
			\centerline{	\includegraphics[width=0.75\textwidth]{\imagedir/univariate/Grades-TS.png}	}
		\column{0.35\textwidth}
			%\onslide+<2->{
			\centerline{	\includegraphics[width=0.75\textwidth]{\imagedir/univariate/Furnace-TS.png}	}
			%}
		\column{0.35\textwidth}
			%\onslide+<3->{
			\centerline{	\includegraphics[width=0.75\textwidth]{\imagedir/univariate/Rand-TS.png}	}
			%}
	\end{columns}
	\begin{columns}[t]
		\column{0.35\textwidth}
			\centerline{	\includegraphics[width=0.75\textwidth]{\imagedir/univariate/Grades-hist.png}	}
		\column{0.35\textwidth}
			%\onslide+<2->{
			\centerline{	\includegraphics[width=0.75\textwidth]{\imagedir/univariate/Furnace-hist.png}	}
			%}
		\column{0.35\textwidth}
			%\onslide+<3->{
			\centerline{	\includegraphics[width=0.75\textwidth]{\imagedir/univariate/Rand-hist.png}	}
			%}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Inverse cumulative distribution function (inverse CDF)}
	\begin{itemize}
		\item	\textbf{Cumulative distribution}: area underneath the distribution function
		\item	\textbf{Inverse cumulative distribution}: we know the area, but want to get back to the value along the $z$-axis.
	\end{itemize}
	\vspace{-4pt}
	\begin{center}
		\includegraphics[width=0.65\textwidth]{\imagedir/univariate/show-pnorm-and-qnorm.png}
	\end{center}
\end{frame}

\begin{frame}\frametitle{Checking for normality using q-q plots}
	\begin{columns}[t]
		\column{0.35\textwidth}
			\textbf{Approach}: {\color{myOrange}{compare properties}}
			\begin{itemize}
				\item	use data from the theoretical distribution 
				\item	compare it with actual data.
			\end{itemize}
			
			\vspace{24pt}
			\onslide+<2->{
				We will assume we have $N$ \emph{actual} observations to check.
			}
			
		\column{0.65\textwidth}
			\onslide+<3->{
			1. Create $N$ observations that are normally distributed:
			\begin{center}
				\includegraphics[width=\textwidth]{\imagedir/univariate/show-pnorm-and-qnorm-qqplot.png}
			\end{center}
			}
	\end{columns}

\end{frame}

\begin{frame}[fragile]\frametitle{Checking for normality using q-q plots}
	
	This software code shows how you would do this in R:
	
	\vspace{24pt}
	
	\begin{lstlisting}[language=R]
N = 10
index <- seq(1, N)            #    1,    2, ...   10
P <- (index - 0.5) / N        # 0.05, 0.15, ... 0.95
theoretical.quantity <- qnorm(P)
# [1] -1.64 -1.04 -0.674 -0.385 -0.126
#      0.125  0.385  0.6744 1.036  1.64
	\end{lstlisting}
	
\end{frame}

\begin{frame}[fragile]\frametitle{Checking for normality using q-q plots}
	2. Standardize the actual data points:
	\begin{itemize}
		\item	subtract the mean, divide by standard deviation
		\item	Sort the data from lowest to highest
		\item	These points should now match the theoretical points from step 1 if they are normally distributed. \emph{{\color{myGreen}{Example}}}:
	\end{itemize}

	\begin{lstlisting}[language=R]
yields <- c(86.2, 85.7, 71.9, ... 86.9, 78.4)
mean.yield <- mean(yields)          # 80.0
sd.yield <- sd(yields)              # 8.35

yields.z <- (yields - mean.yield)/sd.yield
#[1] 0.734  0.674 -0.978  1.82 ... -0.140 0.818 -0.2

yields.z.sorted <- sort(yields.z)
#[1] -1.34 -1.04  -0.978 -0.355 ... 0.734 0.818  1.82

theoretical.quantity   # see prior slide
#[1] -1.64 -1.04  -0.674 -0.385 ... 0.674 1.036  1.64
	\end{lstlisting}
\end{frame}

\begin{frame}\frametitle{Checking for normality using q-q plots}

	3. Plot theoretical quantities against the actual quantities.
	\begin{itemize}
		\item	\small They should form a 45 degree line if truly from this distribution
	\end{itemize}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{\imagedir/univariate/qqplot-derivation.png}
	\end{center}
\end{frame}

\begin{frame}\frametitle{Checking for normality using q-q plots}
	\begin{enumerate}
		\setcounter{enumii}{4}
		\item	Usually we unscale the $y$-axis (the axis with actual data) so we can see the original data values
		\item	The \texttt{car} library: adds 95\% confidence bounds that should contain the data points
	\end{enumerate}
	\vspace{-8pt}
	\begin{center}
		\includegraphics[width=0.9\textwidth]{\imagedir/univariate/qqplot-from-R.png}

	\end{center}

\end{frame}

\begin{frame}\frametitle{Checking for normality. Are these samples normally distributed?}

	\begin{columns}[t]
		\column{0.35\textwidth}
			\centerline{	\includegraphics[width=0.75\textwidth]{\imagedir/univariate/Grades-hist.png}	}
		\column{0.35\textwidth}
			%\onslide+<2->{
			\centerline{	\includegraphics[width=0.75\textwidth]{\imagedir/univariate/Furnace-hist.png}	}
			%}
		\column{0.35\textwidth}
			%\onslide+<3->{
			\centerline{	\includegraphics[width=0.75\textwidth]{\imagedir/univariate/Rand-hist.png}	}
			%}
	\end{columns}
	
	\vspace{10pt}
	\begin{columns}[t]
		\column{0.35\textwidth}
			\centerline{	\includegraphics[width=0.75\textwidth]{\imagedir/univariate/Grades-qqplot.png}	}
		\column{0.35\textwidth}
			%\onslide+<2->{
			\centerline{	\includegraphics[width=0.75\textwidth]{\imagedir/univariate/Furnace-qqplot.png}	}
			%}
		\column{0.35\textwidth}
			%\onslide+<3->{
			\centerline{	\includegraphics[width=0.75\textwidth]{\imagedir/univariate/Rand-qqplot.png}	}
			%}
	\end{columns}
\end{frame}


\begin{frame}\frametitle{Recap of the prior material}
	$z$-value for any variable $x$ is defined as: $z = \dfrac{x - \text{mean}}{\text{standard deviation}}$
	
	\vspace{18pt}
	We were also able to derive confidence interval bounds in the prior class:
	
	\begin{columns}[t]
		\column{0.5\textwidth}
			\begin{itemize}
				\item	$\displaystyle - c_n < \frac{\bar{x} - \mu}{\sigma / \sqrt{n}} < c_n$
		
				\vspace{10pt}
				\item	$\displaystyle \bar{x} - c_n\frac{\sigma}{\sqrt{n}} < \mu < \bar{x} + c_n\frac{\sigma}{\sqrt{n}}$
		
				\vspace{10pt}
				\item	$\displaystyle \text{LB} < \mu < \text{UB}$
			\end{itemize}
		\column{0.5\textwidth}
			% nothing here
	\end{columns}
	
\end{frame}

\begin{frame}\frametitle{Central limit theorem: extended}

	\includegraphics[width=\textwidth]{\imagedir/univariate/CLT-derivation.png}

	\vspace{18pt}
	\textbf{New:} $\bar{x} \sim \mathcal{N}\left(\mu, \sigma^2/n \right)$  \hfill{\color{myGreen}{$\longleftarrow$ really powerful statement}}

	\vspace{18pt}
	\emph{Interpretation}

	Repeated estimates of mean are unbiased (tends towards $\mu$ {\color{myOrange}{$\leftarrow$ of the distribution sampled from!}}) and variance of that estimate is decreased by a factor $n$. 
	
\end{frame}

\begin{frame}\frametitle{If we have $x \sim \mathcal{N}\left(\mu, \sigma^2 \right)$ then we get $\bar{x} \sim \mathcal{N}\left(\mu, \sigma^2/n \right)$}
	\centerline{\includegraphics[width=0.83\textwidth]{\imagedir/univariate/explain-confidence-interval.png}}
\end{frame}

\begin{frame}\frametitle{Relaxing the requirement to know $\sigma$}
	
	\begin{columns}[t]
		\column{0.5\textwidth}
			Our current approach:
			\begin{itemize}
				
				\item	$z = \dfrac{\bar{x} - \mu}{{\color{blue}\sigma} / \sqrt{n}}$
				
				\onslide+<2->{
				\vspace{10pt}
				\item	$z \sim \mathcal{N}\left(\mu, \sigma^2/n \right)$
				}
			\end{itemize}
			\onslide+<3->{
			\vspace{20pt}Raw data are from any distribution of finite variance.}
			
		\column{0.5\textwidth}
			Our revised approach:
			\begin{itemize}
				
				\item	$z = \dfrac{\bar{x} - \mu}{{\color{red}s}/ \sqrt{n}}$
				
				\onslide+<2->{
				\vspace{10pt}
				\item	$z \sim t\left(\nu=n-1 \right)$
				}
				
				
			\end{itemize}
			\onslide+<3->{
			\vspace{20pt}Additionally requires that the raw data are normally distributed.\hfill{\color{myGreen}{$\longleftarrow$  easy to verify}}}
			
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Summary of our revised process}
	\begin{center}
		\includegraphics[width=0.9\textwidth]{\imagedir/univariate/t-distribution-derivation.png}
	\end{center}
	
\end{frame}

\begin{frame}\frametitle{The $t$-distribution is heavily used in this course: let's examine it a bit more}
		\begin{center}
		\includegraphics[width=\textwidth]{\imagedir/univariate/t-distribution-comparison.png}
	\end{center}
	It has one parameter: $z \sim t\left(\nu\right)$, where $\nu = n-1$ = degrees of freedom. Here $\nu=8$.
\end{frame}

% \begin{frame}\frametitle{The $t$-distribution is heavily used in this course: let's examine it a bit more}
% 		\begin{center}
% 		\includegraphics[width=\textwidth]{\imagedir/univariate/t-distribution-comparison-3.png}
% 	\end{center}
% 	It has one parameter: $z \sim t\left(\nu\right)$, where $\nu = n-1$ = degrees of freedom. Here $\nu=3$.
% \end{frame}
% \begin{frame}\frametitle{The $t$-distribution is heavily used in this course: let's examine it a bit more}
% 		\begin{center}
% 		\includegraphics[width=\textwidth]{\imagedir/univariate/t-distribution-comparison-5.png}
% 	\end{center}
% 	It has one parameter: $z \sim t\left(\nu\right)$, where $\nu = n-1$ = degrees of freedom. Here $\nu=5$.
% \end{frame}
% \begin{frame}\frametitle{The $t$-distribution is heavily used in this course: let's examine it a bit more}
% 		\begin{center}
% 		\includegraphics[width=\textwidth]{\imagedir/univariate/t-distribution-comparison-7.png}
% 	\end{center}
% 	It has one parameter: $z \sim t\left(\nu\right)$, where $\nu = n-1$ = degrees of freedom. Here $\nu=7$.
% \end{frame}
% \begin{frame}\frametitle{The $t$-distribution is heavily used in this course: let's examine it a bit more}
% 		\begin{center}
% 		\includegraphics[width=\textwidth]{\imagedir/univariate/t-distribution-comparison-9.png}
% 	\end{center}
% 	It has one parameter: $z \sim t\left(\nu\right)$, where $\nu = n-1$ = degrees of freedom. Here $\nu=9$.
% \end{frame}
% \begin{frame}\frametitle{The $t$-distribution is heavily used in this course: let's examine it a bit more}
% 		\begin{center}
% 		\includegraphics[width=\textwidth]{\imagedir/univariate/t-distribution-comparison-13.png}
% 	\end{center}
% 	It has one parameter: $z \sim t\left(\nu\right)$, where $\nu = n-1$ = degrees of freedom. Here $\nu=13$.
% \end{frame}
% \begin{frame}\frametitle{The $t$-distribution is heavily used in this course: let's examine it a bit more}
% 		\begin{center}
% 		\includegraphics[width=\textwidth]{\imagedir/univariate/t-distribution-comparison-20.png}
% 	\end{center}
% 	It has one parameter: $z \sim t\left(\nu\right)$, where $\nu = n-1$ = degrees of freedom. Here $\nu=20$.
% \end{frame}

\begin{frame}\frametitle{Relaxing the requirement to know $\sigma$}
	
	\begin{columns}[t]
		\column{0.5\textwidth}
			Our current approach:
			\begin{itemize}
				
				\item	$z = \dfrac{\bar{x} - \mu}{{\color{blue}\sigma} / \sqrt{n}}$
				
				\vspace{10pt}
				\item	$z \sim \mathcal{N}\left(\mu, \sigma^2/n \right)$
				
				\onslide+<2->{
				\vspace{10pt}
				\item	$\displaystyle -c_n \leq z \leq +c_n$
				}
				\onslide+<3->{
				\vspace{10pt}
				\item	$\displaystyle -c_n \leq  \dfrac{\bar{x} - \mu}{{\color{blue}\sigma} / \sqrt{n}} \leq +c_n$
				
				\vspace{10pt}
				\item	$\displaystyle \bar{x} - c_n\frac{{\color{blue}\sigma}}{\sqrt{n}} \leq \mu \leq \bar{x} + c_n\frac{{\color{blue}\sigma}}{\sqrt{n}}$
				}
				
				\onslide+<4->{
				\vspace{10pt}
				\item	$17.7 \leq \mu \leq 22.3$
				}
				
			\end{itemize}
			
		\column{0.5\textwidth}
			Our revised approach:
			\begin{itemize}
				
				\item	$z = \dfrac{\bar{x} - \mu}{{\color{red}s}/ \sqrt{n}}$
				
				
				\vspace{10pt}
				\item	$z \sim t\left(\nu=n-1 \right)$
				
				\onslide+<2->{
				\vspace{10pt}
				\item	$\displaystyle -c_t \leq z \leq +c_t$
				}
				\onslide+<3->{
				\vspace{10pt}
				\item	$\displaystyle -c_t \leq  \dfrac{\bar{x} - \mu}{{\color{red}s} / \sqrt{n}} \leq +c_t$
				
				\vspace{10pt}
				\item	$\displaystyle \bar{x} - c_t\frac{{\color{red}s}}{\sqrt{n}} \leq \mu \leq \bar{x} + c_t\frac{{\color{red}s}}{\sqrt{n}}$
				}
				
				\onslide+<4->{
				\vspace{10pt}
				\item	$17.1 \leq \mu \leq 22.9$
				}
			\end{itemize}
			
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Reading values from a table of the $t$-distribution}
	\centerline{\includegraphics[width=\textwidth]{\imagedir/univariate/Statistical-tables/t-distribution-table.png} }
\end{frame}

\begin{frame}\frametitle{Example (continued from the prior videos)}

	Large bale of polymer composite: 9 independent samples taken and sent to lab. The samples are independent estimates of the bale's entire (population) viscosity.
	\begin{itemize}
		\item	$x$ = \texttt{23, 19, 17, 18, 24, 26, 21, 14, 18}
		\item	$\bar{x} = 20.0$
		\item	$s=3.81$
	\end{itemize}
	\myhrule
	
	The $z$-value is \emph{not known} (we do not know $\mu$): find critical lower and upper bounds that will contain 95\% of all possible $z$-values.
	
	\[-c_t \leq z leq +c_t\]
	
	What would the $\pm c_t$ value be for this example?
	
	\vspace{12pt}
	\begin{columns}[t]
		\column{0.8\textwidth}

		\column{0.3\textwidth}
			\onslide+<2->{
				\texttt{qt(0.975, df=8)}
			}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Confidence Intervals: we have to be \textbf{very} comfortable with this concept}

	\textbf{Example}: a customer is evaluating your product: they want a confidence interval (CI) for the impurity level in your sulphuric acid over the past year.

	\vspace{12pt}

	\onslide+<2->{
	{\color{myOrange}\emph{Response}}: the range from 429ppm to 673ppm contains the true impurity level with 95\% confidence.
	
	\vspace{12pt}

	This is a compact representation of the impurity level.
	}
	
	\onslide+<3->{
	$\qquad$

	\emph{Alternatively} you could have said:
	\begin{itemize}
		\item	the sample mean from the last year of data is 551 ppm
		\item	the sample standard deviation from the last year [can be back-calculated if we were told $n$]
		\item	the last year of data are normally distributed
	\end{itemize}

	A CI conveys this information equally well, and in a useful manner.
	}
\end{frame}

\begin{frame}\frametitle{Summary: confidence interval derivation for the case where the variance, $\sigma^2$, is not known}

	%Previous example: the interval that contains the population viscosity was:

	$
	\begin{array}{rcccl}
		- c_t &\leq& \displaystyle \frac{\bar{x} - \mu}{s/\sqrt{n}} &\leq& +c_t\\
		\bar{x} - c_t \displaystyle \frac{s}{\sqrt{n}} &\leq& \mu &\leq& \bar{x} + c_t\displaystyle\frac{s}{\sqrt{n}} \\
		\text{LB} &\leq& \mu &\leq& \text{UB}
	\end{array}
	$
	
	\vspace{20pt}
	$
	\begin{array}{rcl}
		\text{LB} &= \bar{x} - c_t \displaystyle \frac{s}{\sqrt{n}}\\
		\text{UB} &= \bar{x} + c_t \displaystyle \frac{s}{\sqrt{n}}
	\end{array}
	$
	
	\vspace{20pt}
	Do you recall the important assumptions that lead to these results?
\end{frame}

\begin{frame}\frametitle{Interpreting the {\color{purple}{confidence interval (CI)}} {\color{red} incorrectly}}
	
	\begin{itemize}
		\item	$x$ = \texttt{23, 19, 17, 18, 24, 26, 21, 14, 18}
		\item	$\bar{x} = 20.0$
		\item	$s=3.81$
		\item	$c_t = 2.31$ for the 95\% confidence interval with 8 degrees of freedom
		\item	LB = $20.0 - 2.92 = 17.1$
		\item	UB = $20.0 + 2.92 = 22.9$
	\end{itemize}
	
	\myhrule
	\begin{itemize}
		\onslide+<2->{
			\item	The CI {\color{myOrange}{\textbf{does not imply}}} that $\bar{x}$ lies in the interval from LB to UB, i.e. the CI is not about $\bar{x}$; it's about $\mu$
		}
		\onslide+<3->{
			\item	{\color{red}\textbf{Incorrect}} to say: (sample) average viscosity is 20 units and lies inside the range of 17.1 to 22.9 with a 95\% probability
		}

	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Interpreting the {\color{purple}{confidence interval (CI)}} {\color{blue} correctly}}
	
	\begin{itemize}
		\item	LB = $20.0 - 2.92 = 17.1$
		\item	UB = $20.0 + 2.92 = 22.9$
	\end{itemize}
	
	\myhrule
	\begin{itemize}
		\item	The CI \textbf{does imply} that $\mu$ is expected to lie within that interval, with the given level of confidence
		
		\onslide+<2->{
			\item	The {\color{myGreen}{\textbf{CI is a range}}} of possible values for $\mu$, not for $\bar{x}$.
		}
		\onslide+<3->{
			\item	UB and LB are a function of the data sample
			\item	If we take a different sample of data, we will get different bounds
		}
		\onslide+<4->{
		\begin{itemize}
			\item	If confidence level is 95\%, then 5\% of the time the interval \emph{will not contain} the true mean
			\item	Collect 20 sets of samples, 19 times out of 20 the CI range will contain the true mean
			\onslide+<5->{
				\item	\textbf{IT IS NOT}: Probability that the true population viscosity, $\mu$ is within the given range {\color{myOrange}{[{\scriptsize $\mu$ doesn't have a probability; it is 100\% fixed}]}}
				\item	\textbf{IT IS}: Probability that the \emph{CI range} contains the true population viscosity, $\mu$
			}
		\end{itemize}
		}
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Interpreting the confidence interval by altering the sample, process, or confidence level}

	\[
		\begin{array}{rcccl}
			\bar{x} - c_t \dfrac{s}{\sqrt{n}} &\leq& \mu &\leq& \bar{x} + c_t\dfrac{s}{\sqrt{n}} \\
		\end{array}
	\]
	\begin{itemize}
		\item	As $n$ increases, the confidence interval range decreases
		\item	but with diminishing returns (intuitively expected)
	\end{itemize}
	
	\vspace{10pt}
	\onslide+<2->{
		\begin{itemize}
			\item	We could have adjusted $s$ as well.
		\end{itemize}
	}
	\onslide+<3->{
		\begin{itemize}
			\item	We can arbitrarily select our level of confidence:
		\end{itemize}
		\begin{center}
			\begin{tabular}{c|cc}
				\textbf{Confidence level} & \textbf{LB} & \textbf{UB}\\ \hline
				90\%	& 17.6	&	22.4\\
				95\%	& 17.1	&	22.9\\
				99\%	& 15.7	&	24.2\\
			\end{tabular}
		\end{center}
	}
	\onslide+<4->{
		\begin{itemize}
			\item	What happens if the level of confidence is 100\%?
			\begin{itemize}
				\item	The confidence interval is then infinite.
			\end{itemize}
		\end{itemize}
	}
\end{frame}

\begin{frame}\frametitle{Summary: confidence for interval for the parameter $\mu$, the mean}
	Make an assumption about the raw data:
	\begin{itemize}
		\item	$n$ independent points \qquad {\color{myOrange}{$\longleftarrow$ this assumption is always required}}
	\end{itemize}
	
	\vspace{12pt}
	

	\begin{columns}[t]
		\column{0.5\textwidth}
			\textbf{We know the population $\sigma$}
			\[
			 	z = \dfrac{\bar{x} - \mu}{\sigma/\sqrt{n}}
			 \]			
			 
			 \vspace{20pt}

			\begin{itemize}
				\item	$\bar{x} \sim \mathcal{N}(\mu, \sigma^2/n)$
								
				\vspace{12pt}
				\item	$\displaystyle \bar{x} - c_n\frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{x} + c_n\frac{\sigma}{\sqrt{n}}$
				
				\vspace{12pt}
				\item	\texttt{qnorm(0.975)} when at the 95\% confidence level
			\end{itemize}
		\column{0.5\textwidth}
			\textbf{We do not know the population $\sigma$}
			\[
				 z = \dfrac{\bar{x} - \mu}{s/\sqrt{n}}
			\]
			\begin{itemize}
				\item	{\color{myGreen}Also, we require $x \sim \mathcal{N}(\mu, \sigma^2/n)$}
				\item	$\bar{x} \sim t(\nu = n-1)$
				
				\vspace{12pt}
				\item	$\displaystyle \bar{x} - c_t\frac{s}{\sqrt{n}} \leq \mu \leq \bar{x} + c_t\frac{s}{\sqrt{n}}$
				
				\vspace{12pt}
				
				\item	\texttt{qt(0.975, df=...)} when at the 95\% confidence level
			\end{itemize}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Comparison between the two cases: how does it matter?}

	You can prove the confidence interval with unknown variance is wider. (600-level students ... why?)
	\vspace{12pt}
	We expect this intuitively:
	\begin{itemize}
		\item	it reflects our uncertainty of the spread parameter: $s$ vs $\sigma$
		\item	leads to a more conservative result (i.e. wider bound)
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Many other confidence intervals exist}
	\begin{itemize}
		\item	They exist for population variances: $\text{LB} \leq \sigma^2 \leq \text{UB}$
		\begin{itemize}
			\item	ratio of two variances: \emph{do these 2 samples have same variance}?
		\end{itemize}

		\vspace{12pt}
		\item	They exist for proportions:
		\begin{itemize}
			\item	proportion of \emph{packaged pizzas with N or more pepperoni slices is between 86 and 92\%}
			\item	political polls: \emph{party XYZ has 35\% of the vote, the margin of error is 3\%; accurate 19 times out of 20}
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{A worked example}
	Your manager is asking for the average viscosity of a product that you produce in a batch process. 
	
	\begin{columns}[T]
		\column{0.5\textwidth}
			\vspace{10pt}
			Recorded below are the 12 most recent values, taken from consecutive batches. State any assumptions, and clearly show the calculations which are required to estimate a 95\% confidence interval for the mean. 
	
			\vspace{10pt}
			Interpret that confidence interval for your manager, who is not sure what a confidence interval is.
	
			\vspace{10pt}
			13.7, 14.9, 15.7, 16.1, 14.7, 15.2, 
			
			13.9, 13.9, 15.0, 13.0, 16.7, 13.2
		\column{0.4\textwidth}
			\centerline{\includegraphics[width=\textwidth]{\imagedir/batch/flickr-batch-reactor-2516220152_074fbbb489_o.jpg} }
			\see{\href{http://www.flickr.com/photos/polapix/2516220152/}{Flickr: 2516220152}}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{A worked example: what assumptions are being made?}
	\begin{itemize}
		\item	We can assume the sample average $\bar{x} \sim \mathcal{N}$
		\onslide+<2->{
			\item	We don't know $\sigma$; so we must estimate it. 
		}
		\onslide+<3->{
			\item	That means $z = \dfrac{\bar{x} - \mu}{s / \sqrt{n}} \sim t(\nu)$
		}
		\onslide+<4->{
			\item	which in turn requires us to assume the raw data, $x_i \sim \mathcal{N}$
		}
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{A worked example: lets do the calculations and interpret the result}
	\begin{itemize}
		\item	$\displaystyle \bar{x} - c_t\frac{s}{\sqrt{n}} \leq \mu \leq \bar{x} + c_t\frac{s}{\sqrt{n}}$
		\item	$s =$ \onslide+<2->{1.16}
		\item	$c_t =$ \onslide+<2->{2.2}
		
		\vspace{12pt}
		\onslide+<3->{
			\item	$14.67 - \dfrac{2.2 \times 1.16}{\sqrt{12}} \leq \mu \leq 14.67 + \dfrac{2.20 \times 1.16}{\sqrt{12}} $
			\item	$13.9 \leq \mu \leq 15.4$
		}
		\vspace{24pt}
		\onslide+<4->{
			\item	\textbf{Interpretation}: the range has a 95\% probability of containing the true viscosity.
		}
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{A worked example: a caution}
	\begin{columns}[T]
		\column{0.5\textwidth}
			
			We also need to clean the batch reactor properly between every use. 
			
			\vspace{12pt}
			There are other ways that independence can be violated: what about raw material lots and operators running the batch?
			
			\vspace{12pt}
			In practice, we will definitely violate this assumption to some extent.
		\column{0.4\textwidth}
			\centerline{\includegraphics[width=\textwidth]{\imagedir/batch/flickr-batch-reactor-2516220152_074fbbb489_o.jpg} }
			\see{\href{http://www.flickr.com/photos/polapix/2516220152/}{Flickr: 2516220152}}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{The batch yield feedback case study}
	Your manager is asking for the average viscosity of a product that you produce in a batch process. 
	
	\begin{columns}[T]
		\column{0.53\textwidth}
			
			\begin{itemize}
				\item	Most recent 10 batches run, using the existing system (A): average yield is $\bar{x}_A =$ {\color{blue}79.9\%}
				
				\vspace{18pt}
				\onslide+<2->{
					\item	Put on a prototype control system (B) and run 10 batches: $\bar{x}_B =$ {\color{blue}82.9\%}
				}
				\vspace{18pt}
				\onslide+<3->{
					\item	$\bar{x}_B - \bar{x}_A = 82.93 - 79.89 = 3.04\%$
				}
				\vspace{18pt}
				\onslide+<4->{
					\item	Was this just lucky, or is this a permanent improvement? What's the risk we are wrong?
				}
			\end{itemize}
		\column{0.4\textwidth}
			\centerline{\includegraphics[width=\textwidth]{\imagedir/batch/flickr-batch-reactor-2516220152_074fbbb489_o.jpg} }
			\see{\href{http://www.flickr.com/photos/polapix/2516220152/}{Flickr: 2516220152}}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{Let's look at the raw data: table form}
	\includegraphics[width=\textwidth]{\imagedir/univariate/system-comparison-wikitable.png}
\end{frame}

\begin{frame}\frametitle{Let's look at the raw data: box plot of the data}
	\includegraphics[width=\textwidth]{\imagedir/univariate/system-comparison-boxplot-plots.png}
\end{frame}

\begin{frame}\frametitle{Let's look at the raw data: given the context of prior batches}
	\includegraphics[width=\textwidth]{\imagedir/univariate/system-comparison-sequence-plot-slides.png}
\end{frame}

\begin{frame}\frametitle{One approach: compare the data to a reference set}
	\begin{enumerate}
		\item	We have 300 previous batches operating with system A
		\begin{itemize}
			\item	Calculate average from batch 1, 2, 3, ... 10
			\item	Calculate average from batch 11, 12, 13, ... 20
		\end{itemize}
		\item	Subtract averages: (group average 11 to 20) minus (group average 1 to 10)
		\item	Shift and repeat steps 2 and 3: use batches (2 to 11) and (12 to 21)
		\item	Collect all the difference values
	\end{enumerate}
\end{frame}

\begin{frame}\frametitle{The question we want to answer}
	Let's assume the new system B is no different. 
	
	We assume that 3\% increase was pure luck.
	
	\vspace{36pt}
	\begin{exampleblock}{}
		{\color{myOrange} How many times, in the past, did we observe 3\% improvements between two sets of 10 consecutive batches?}
	\end{exampleblock}
	
	If our assumption at the top of the slide is correct, we should not see many 3\% improvements.

	
\end{frame}

\begin{frame}\frametitle{A good visualization tool: the dot plot}

	\includegraphics[width=\textwidth]{\imagedir/univariate/system-comparison-dotplot-grouped.png}
	
	\onslide+<2->{
		\begin{itemize}
			\item	31 historical differences out of 281 had a difference value higher than 3.04
			\item	11\% of historical batches had a better difference, by chance, not due to the feedback controller change.
		\end{itemize}
	}
	\onslide+<3->{
		\textcolor{red}{No assumption of independence, nor any form of distribution assumed!}
	}
	
	\vspace{5pt}
	\onslide+<4->{
		\textcolor{myGreen}{Advanced students}: plot a time-series plot of these data, for an even greater insight to what is occurring here.
	}
\end{frame}

\begin{frame}\frametitle{Has a significant change occurred in the system?}
	\begin{exampleblock}{}
		Our key focus this time: \textbf{we have limited data.} We say, 
		
		\qquad ``\emph{there is no external reference set available}''.
	\end{exampleblock}
	
	\vspace{24pt}
	In the prior example, we have plenty of historical data:
	
	\includegraphics[width=0.6\textwidth]{\imagedir/univariate/system-comparison-sequence-plot-slides.png}
\end{frame}

\begin{frame}\frametitle{Second approach: when no reference data is available}
	\begin{itemize}
		\item	Don't have a \textbf{\emph{suitable}} reference, e.g. just the 20 runs
		\item	We will take a look at the method, and assumptions
	\end{itemize}

	\textbf{Aim}: is $\mu_B > \mu_A$ ? In other words, is $\mu_B - \mu_A > 0$?
	\begin{enumerate}
		\item	Assume data for case A is normally distributed: A $\sim \mathcal{N}\left(\mu_A, \sigma^2_A\right)$
		
		\item	Assume data for case B is normally distributed: B $\sim \mathcal{N}\left(\mu_B, \sigma^2_B\right)$
		\item	Assume data for sample A and sample B have $\sigma_A = \sigma_B = \sigma$
		\item	From CLT (assumes data in A and data in B are independent)%:  {\color{myOrange}{\small $\longleftarrow$ but, but ... we know that's not true here!}}
			\begin{itemize}
				\item	$\mathcal{V}\left\{\bar{x}_A\right\} = \dfrac{\sigma^2_A}{n_A}$
				\item	$\mathcal{V}\left\{\bar{x}_B\right\} = \dfrac{\sigma^2_B}{n_B}$
			\end{itemize}
	\end{enumerate}
\end{frame}

\begin{frame}\frametitle{Second approach: when no reference data is available}

	5. Assume: $\bar{x}_A$ and $\bar{x}_B$ are independent [\emph{likely true in many cases}] %, then from your prior stats course you should know that ...
	\begin{itemize}
		\item	$\mathcal{V}\left\{\bar{x}_B - \bar{x}_A\right\} = \dfrac{\sigma^2}{n_A} + \dfrac{\sigma^2}{n_B} = \sigma^2 \left(\dfrac{1}{n_A} + \dfrac{1}{n_B}\right)$
	\end{itemize}

	6. Create a $z$-value:
	
	\vspace{10pt}
	\begin{itemize}
		\item	$z = \dfrac{\left(\text{variable ``$x$''}\right) - \left(\text{``location''}\right)}{\text{``spread''}} = \dfrac{(\bar{x}_B - \bar{x}_A) - (\mu_B - \mu_A)}{\sqrt{\sigma^2 \left(\dfrac{1}{n_A} + \dfrac{1}{n_B}\right)}}$
	\end{itemize}

	7. Create a confidence interval for $z$
	
	\vspace{10pt}
	{${\color{blue} (\bar{x}_B - \bar{x}_A) - c_n \sqrt{\sigma^2 \left(\tfrac{1}{n_A} + \tfrac{1}{n_B}\right)}  } < {\color{red} \mu_B - \mu_A} < {\color{blue} (\bar{x}_B - \bar{x}_A) + c_n \sqrt{\sigma^2 \left(\tfrac{1}{n_A} + \tfrac{1}{n_B} \right)} }$}
	
	
\end{frame}

\begin{frame}\frametitle{Back to method 2 for testing differences}

	\textbf{Aim}: is $\mu_B > \mu_A$ ? In other words, is $\mu_B - \mu_A > 0$?

	1. Assume sample A

	2. Assume equal variance: $\sigma_A = \sigma_B = \sigma$

	3. So from CLT:
	\begin{itemize}
		\item	$\mathcal{V}\left\{\bar{x}_A\right\} = \dfrac{\sigma^2_A}{n_A}$ and $\mathcal{V}\left\{\bar{x}_B\right\} = \dfrac{\sigma^2_B}{n_B}$
	\end{itemize}

	4. Assume: $\bar{x}_A$ and $\bar{x}_B$ are independent - so combine variances:
	\begin{itemize}
		\item	$\mathcal{V}\left\{\bar{x}_B - \bar{x}_A\right\} = \dfrac{\sigma^2}{n_A} + \dfrac{\sigma^2}{n_B} = \sigma^2 \left(\dfrac{1}{n_A} + \dfrac{1}{n_B}\right)$
	\end{itemize}

	6. Create $z$-value. Ask: what is our probability/risk?
	\begin{itemize}
		\item$z = \dfrac{(\bar{x}_B - \bar{x}_A) - (\mu_B - \mu_A)}{\sqrt{\sigma^2 \left(\dfrac{1}{n_A} + \dfrac{1}{n_B}\right)}}$
	\end{itemize}

	
\end{frame}

\begin{frame}\frametitle{Method 2b: No reference set, internal $\sigma$}
	\begin{itemize}
		\item	Sample variances: $s_A^2 = 6.81^2$ and $s_B^2 = 6.70^2$
		\item	It just happens that $n_A = n_B = 10$ (can have $n_A \neq n_B$)
		\item	Pool (combine) the variance, using a weighted sum:
	\end{itemize}

	$
	\begin{array}{rcccl}
		s_P^2 &=& \dfrac{(n_A -1) s_A^2 + (n_B-1)s_B^2}{n_A - 1 + n_B - 1} \\ \\ 
		s_P^2 &=& \dfrac{9\times 6.81^2 + 9 \times 6.70^2}{18} = 45.63
	\end{array}
	$

	\vspace{8pt}
	$
	\begin{array}{rcccl}
		z &=& \dfrac{\left(\bar{x}_B - \bar{x}_A\right)- (\mu_B - \mu_A)}{\sqrt{s_P^2 \left(\dfrac{1}{10} + \dfrac{1}{10}\right)}} \\
		\\
		z &=& \dfrac{3.04 - 0}{\sqrt{45.63 \times 2/10}} = 1.01
	\end{array}
	$
	
	
	\vspace{8pt}
	Probability of $z < 1.01$? The area from $-\infty$ up to 1.01 is 83.7\% using the \textbf{\emph{t distribution}}.

	Chance of $\bar{x}_B - \bar{x}_A > 3.04$ is about 16\%. System B's performance could have been obtained by pure luck in 16\% of cases.
\end{frame}

\begin{frame}\frametitle{title}
	\vspace{10pt}
	{${\color{blue} (\bar{x}_B - \bar{x}_A) - c_t \sqrt{s_P^2 \left(\tfrac{1}{n_A} + \tfrac{1}{n_B}\right)}  } < {\color{red} \mu_B - \mu_A} < {\color{blue} (\bar{x}_B - \bar{x}_A) + c_t \sqrt{s_P^2 \left(\tfrac{1}{n_A} + \tfrac{1}{n_B} \right)} }$}
	
\end{frame}

\begin{frame}\frametitle{Example of testing for differences: impellers}

	
	% \begin{center}
	% 		\includegraphics[width=0.55\textwidth]{\imagedir/univariate/Mixing_-_flusso_assiale_e_radiale.jpg}
	% 	\end{center}
	\vspace{-6pt}
	Try these cases:
	
	\begin{itemize}
		\item	$43 \,\text{min} < \mu_\text{Axial} - \mu_\text{Radial} < 95\, \text{min} $
		\item	$-95 \,\text{min} < \mu_\text{Radial} - \mu_\text{Axial} < -43\, \text{min} $
		\item	$-12 \,\text{min} < \mu_\text{Axial} - \mu_\text{Radial} < -7\, \text{min} $
		\item	$-453 \,\text{min} < \mu_\text{Axial} - \mu_\text{Radial} < 284\, \text{min} $
		\item	$-21 \,\text{min} < \mu_\text{Axial} - \mu_\text{Radial} < 187\, \text{min} $
	\end{itemize}

\vspace{24pt}
	What is your recommendation for choosing the one impeller over the other?
	
	\vspace{12pt}

	Consider the case of \emph{statistical difference} vs \emph{engineering difference}
\end{frame}

\begin{frame}\frametitle{General principle when testing for significant differences}
	\begin{exampleblock}{}
		Keep everything as constant as possible, except the feature under investigation
	\end{exampleblock}
	
	\onslide+<2->{
		\vspace{12pt}
		In the section on ``Designed Experiments'' we will vary more than 1 item.
	}
	\onslide+<3->{
		\begin{columns}[T]
			\column{0.6\textwidth}
				\includegraphics[width=1.1\textwidth]{\imagedir/univariate/system-comparison-boxplot-plots.png}
				
			\column{0.05\textwidth}				
			\column{0.35\textwidth}
				\centerline{\includegraphics[width=\textwidth]{\imagedir/batch/flickr-batch-reactor-2516220152_074fbbb489_o.jpg} }
				\see{\href{http://www.flickr.com/photos/polapix/2516220152/}{Flickr: 2516220152}}

		\end{columns}
	}
	
	
\end{frame}

\begin{frame}\frametitle{The key equation derived in the prior videos on confidence intervals}
	
	\[
		\begin{array}{rcccl}
			{\color{blue} (\bar{x}_B - \bar{x}_A) - c_t \sqrt{s_P^2 \left(\tfrac{1}{n_A} + \tfrac{1}{n_B}\right)}  } &\leq& {\color{red} \mu_B - \mu_A} &\leq& {\color{blue} (\bar{x}_B - \bar{x}_A) + c_t \sqrt{s_P^2 \left(\tfrac{1}{n_A} + \tfrac{1}{n_B} \right)} } \\ \\
			\text{lower bound} &\leq& {\color{red} \mu_B - \mu_A} &\leq& \text{upper bound}
		\end{array}
	\]

	\vspace{12pt}
	$s_P \triangleq$ the pooled standard deviation
\end{frame}

\begin{frame}\frametitle{}
	\begin{columns}[T]
		\column{0.53\textwidth}
			{\color{myOrange}4 000 L reactor}
			
			\vspace{-12pt}
			\includegraphics[height=\textheight]{\imagedir/univariate/Wikipedia-Agitated_vessel.svg.png}
			
			\onslide+<2->{
				\vspace{-20pt}
				\see{Image credits: \href{http://en.wikipedia.org/wiki/Baffle_(heat_transfer)}{Wikipedia} (left) and \href{https://www.flickr.com/photos/adavey/13193170764/in/photostream/}{Flickr: 13193170764} (right)}
			}
			
		\column{0.4\textwidth}
			
			\onslide+<2->{
				{\color{myOrange}Raw material size: 8 000 L}
				\includegraphics[height=0.95\textheight]{\imagedir/univariate/flickr-13193170764_49d81746c3_k.jpg}
			}
	\end{columns}
\end{frame}

\begin{frame}\frametitle{For paired testing: always be clear on your goal}
	
	\begin{exampleblock}{}
		\color{myGreen}In this example: \emph{what is the effect on conversion, either with baffles or without baffles}?
	\end{exampleblock}
	
	\onslide+<2->{
	\vspace{12pt}
	Our hope for this test (these data are guesses; we have not done the work yet)
	
	\vspace{12pt}
	\vspace{12pt}
	\begin{columns}[T]
		\column{0.0\textwidth}
		\column{0.3\textwidth}
			\centerline{{\color{myOrange}Without baffles} [4 000 L]}
			
			\vspace{12pt}
			\begin{enumerate}
				\setcounter{enumi}{1}
				\item	conversion: 72\%
			\end{enumerate}
			\begin{enumerate}
				\setcounter{enumi}{3}
				\onslide+<4->{\item	conversion: 74\%}
			\end{enumerate}
			\begin{enumerate}
				\setcounter{enumi}{5}
				\onslide+<5->{\item	conversion: 70\%}
			\end{enumerate}
			
			
		\column{0.3\textwidth}
			\centerline{{\color{myOrange}With baffles} [4 000 L]}
			
			\vspace{12pt}
			\begin{enumerate}
				\setcounter{enumi}{0}
				\item	conversion: 78\%
			\end{enumerate}
			\begin{enumerate}
				\setcounter{enumi}{2}
				\onslide+<4->{\item	conversion: 78\%}
			\end{enumerate}
			\begin{enumerate}
				\setcounter{enumi}{4}
				\onslide+<5->{\item	conversion: 75\%}
			\end{enumerate}



		\column{0.3\textwidth}
			\centerline{{\color{myOrange}Difference in results}}
			
			\vspace{12pt}
			\begin{description}
			  \onslide+<3->{\item[Batch $1 - 2$:\quad] 6\%}
			\end{description}
			
			\vspace{0pt}
			\begin{description}
			  \onslide+<4->{\item[Batch $3 - 4$:\quad] 4\%}
			\end{description}
			
			\vspace{0pt}
			\begin{description}
			 \onslide+<5->{ \item[Batch $5 - 6$:\quad] 5\%}
			\end{description}

	\end{columns}
	}
	
	\vspace{15pt}
	\onslide+<6->{\emph{\color{myGreen}We are going to analyze our actual data like this, later on.}}
\end{frame}

\begin{frame}\frametitle{Why should we go to all this trouble of setting up the experiments?}
	
	The effect of raw materials is consistent; we want to cancel it out.
	
	\vspace{24pt}
	Assume the raw materials have an effect of $h$\onslide+<3->{; we will use a value of $h=3\%$ here.}
	
	\vspace{24pt}
	\begin{columns}[T]
		\column{0.03\textwidth}
		\column{0.4\textwidth}
			\onslide+<2->{
			\centerline{{\color{myOrange}Without baffles} }
			}
			\vspace{12pt}
			\begin{itemize}
				\setcounter{enumi}{1}
			\onslide+<2->{
				\item	True conversion = 71\%
			}
			\onslide+<3->{
				\item	Bias of $+h$ = 3\%
			}
			\onslide+<4->{	
				\item	Conversion recorded = 74\%
			}
			\end{itemize}
		
		
		\column{0.4\textwidth}
		\onslide+<5->{	
			\centerline{{\color{myOrange}With baffles}}
			\vspace{12pt}
			\begin{itemize}
				\setcounter{enumi}{1}
				\item	True conversion = 75\%
				\item	Bias of $+h$ = 3\%
				\item	Conversion recorded = 78\%
			\end{itemize}
		}

		\column{0.3\textwidth}
		\onslide+<6->{
			\centerline{{\color{myOrange}Difference in results}}
			
			
			
			\vspace{46pt}
			\begin{itemize}
				\item	$78 - 74 = 4\%$
				
						$(75+h)- (71+h)$
						
						$75 - 71 +h- h$
						
						$ = 4\%$
			\end{itemize}
		}
	\end{columns}
	
	\onslide+<7->{
		\vspace{6pt}
		
		{\color{myGreen}A common, constant source of variation is removed, \\
		and we don't need to know the constant!}
	}
\end{frame}

\begin{frame}\frametitle{When should you use a paired test?}

	\begin{exampleblock}{}
			A paired test is appropriate when there is something in common {\color{red}\emph{within}} pairs of samples in group A and B.
			
			\vspace{12pt}
			But the commonality is not {\color{red}\emph{between}} the pairs.
	\end{exampleblock}
	

	\onslide+<2->{
		\vspace{12pt}
		In our example:
		\begin{itemize}
			\item	{\color{red}\emph{within}}-pair commonality is the raw material
		\end{itemize}
	}

\end{frame}

\begin{frame}\frametitle{How \textbf{not} to run paired experiments}

	1. \textbf{Sequentially assign A and B} {\color{myOrange}{(don't ever do this!)}}

	\vspace{12pt}
	$
	\begin{array}{cccccccccc}
		\text{A} & \text{A} & \text{A} & \text{A} & \text{A} & \text{B} & \text{B} & \text{B} & \text{B} & \text{B} \\
		1 & 1 & 2 & 2 & 3 & 3 & 4 & 4 & 5 & 5\\
	\end{array}
	$
	
	\onslide+<2->{
		\begin{itemize}
			\item	{\color{myGreen}This will certainly lead to confounding with an unintentional factor}
		\end{itemize}
	}

	\vspace{24pt}
	
	\onslide+<3->{
		2. \textbf{Randomly assign A and B} {\color{myOrange}{(commonality between pairs won't cancel)}}

		\vspace{12pt}
		$
		\begin{array}{cccccccccc}
			\text{A} & \text{A} & \text{B} & \text{B} & \text{B} & \text{A} & \text{A} & \text{B} & \text{A} & \text{B} \\
			1 & 1 & 2 & 2 & 3 & 3 & 4 & 4 & 5 & 5\\
		\end{array}
		$
	}
\end{frame}

\begin{frame}\frametitle{Paired tests: so how do I analyze data from such a test?}

	
	\begin{enumerate}
	
		\item	Calculate the differences: $w = [w_1, w_2, \ldots, w_n]$
		\begin{itemize}
	 		\item	There are $n$ differences (and $2n$ experiments that were done)
			\item	$w_i = x_{A,i} - x_{B,i}$  \qquad\emph{or}\qquad $w_i = x_{B,i} - x_{A,i}$
			\item	just be consistent in your calculation \emph{and} then your interpretation
	 		\item	\textbf{Our main goal}: are the differences significant, or are they essentially zero?
	 		\item	This is something that confidence intervals do well
	 	\end{itemize}
	\pause

		\vspace{12pt}
	 	\item	Calculate the mean difference, $\overline{w}$
		
	\pause
		\vspace{12pt}
	 	\item	$\overline{w}$ should be normally distributed (CLT): $\overline{w} \sim \mathcal{N}\left(\mu_w, \sigma_w^2/n \right)$
				\begin{itemize}
		 			\item	$\mu_w = \mu_A - \mu_B$  (if you calculated $w_i = x_{A,i} - x_{B,i}$)
		 			\item	$\mu_w = \mu_B - \mu_A$  (if you calculated $w_i = x_{B,i} - x_{A,i}$)
		 		\end{itemize}

	\end{enumerate}

\end{frame}

\begin{frame}\frametitle{Paired tests: so how do I analyze data from such a test?}
	\begin{enumerate}
		\setcounter{enumi}{3}
		\item	Calculate the $z$-value for the confidence interval. 
		
		\begin{itemize}
			\item	Use the sample standard deviation (implies $t$-distribution)
			\item	standard deviation = $s_{\overline{w}}$
			
			\vspace{12pt}
			\item	$z = \dfrac{\overline{w} - \mu_w}{s_{\overline{w}} / \sqrt{n}}$
			\vspace{6pt}
			\item	$z \sim t(\nu=n-1)$
		\end{itemize}
		
		\pause
		\item	\[-c_t \leq z \leq + c_t\]
		\begin{itemize}
			\item	$c_t$ critical value from $t$-distribution with $n-1$ DOF at the level of confidence you want.
			\item	Use \texttt{qt(...)} function to find $c_t$
		\end{itemize}
		
		\pause
		\item	\[\overline{w} - c_t \dfrac{s_{\overline{w}}}{\sqrt{n}} \leq \mu_w \leq \overline{w} + c_t \dfrac{s_{\overline{w}}}{\sqrt{n}}\]
	\end{enumerate}
\end{frame}

\begin{frame}\frametitle{Results from the example}
	\begin{itemize}
		\item	$\overline{w} =  -1.6$
		\item	$c_t = 2.78$
		\item	$s_{\overline{w}} = 4.28$
		\item	$n=5$
		
			\[\overline{w} - c_t \dfrac{s_{\overline{w}}}{\sqrt{n}} \leq \mu_w \leq \overline{w} + c_t \dfrac{s_{\overline{w}}}{\sqrt{n}}\]
			\[-1.6 - 2.78 \dfrac{4.28}{\sqrt{5}} \leq \mu_w \leq -1.6 + 2.78 \dfrac{4.28}{\sqrt{5}}\]
			\[-1.6 - 5.3 \leq \mu_w \leq -1.6 +5.3\]
			
			\[-6.9 \leq \mu_w \leq +3.7\]
	\end{itemize}
	
	\onslide+<2->{
		$\mu_w = \mu_B - \mu_A$	
	}
\end{frame}

\begin{frame}\frametitle{Going beyond the statistics}
	Changes and improvements to a process often impact other outcomes.
	
	\begin{exampleblock}{}
		Always consider the secondary effects of the change you made.
	\end{exampleblock}
	
	\vspace{12pt}
	\begin{itemize}
		\item	A result of ``{\color{blue}not significant change}'' can be a good thing sometimes.
		\item	It means you can ignore what you originally were testing for, and make other improvements.
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Paired tests: some important assumptions required }

	\begin{itemize}
		\item	the $n$ paired differences must be normally distributed, not the raw data
		\item	we only have to ensure the pairs are independent of each other
	\end{itemize}

\onslide+<2->{
	\vspace{12pt}
	Paired tests must be intentionally planned. You cannot retroactively analyze data as if they were paired.
}
\end{frame}

